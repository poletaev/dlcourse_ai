{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.ones((3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.ones((2, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train[:, None] - test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.06106005e-09 4.53978686e-05 9.99954600e-01]\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "print(probs)\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs: [0.57611688 0.21194156 0.21194156]\n",
      "loss: 1.551444713932051\n",
      "probs: [0.57611688 0.21194156 0.21194156]\n",
      "loss: 1.551444713932051\n",
      "probs: [0.57611444 0.21194278 0.21194278]\n",
      "loss: 1.5514389527754138\n",
      "probs: [0.57611933 0.21194034 0.21194034]\n",
      "loss: 1.551450475113109\n",
      "probs: [0.57611811 0.21193989 0.21194201]\n",
      "loss: 1.5514525945248259\n",
      "probs: [0.57611566 0.21194323 0.21194111]\n",
      "loss: 1.5514368333559785\n",
      "probs: [0.57611811 0.21194201 0.21193989]\n",
      "loss: 1.551442594524826\n",
      "probs: [0.57611566 0.21194111 0.21194323]\n",
      "loss: 1.5514468333559783\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs: [0.57611688 0.21194156 0.21194156]\n",
      "loss: 1.551444713932051\n",
      "probs: [0.57611688 0.21194156 0.21194156]\n",
      "loss: 1.551444713932051\n",
      "probs: [0.57611444 0.21194278 0.21194278]\n",
      "loss: 1.5514389527754138\n",
      "probs: [0.57611933 0.21194034 0.21194034]\n",
      "loss: 1.551450475113109\n",
      "probs: [0.57611811 0.21193989 0.21194201]\n",
      "loss: 1.5514525945248259\n",
      "probs: [0.57611566 0.21194323 0.21194111]\n",
      "loss: 1.5514368333559785\n",
      "probs: [0.57611811 0.21194201 0.21193989]\n",
      "loss: 1.551442594524826\n",
      "probs: [0.57611566 0.21194111 0.21194323]\n",
      "loss: 1.5514468333559783\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2., -1.,  1.],\n",
       "       [ 1.,  2., -1., -1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.max(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions -= m[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  0., -3., -1.],\n",
       "       [-1.,  0., -3., -3.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36787944, 1.        , 0.04978707, 0.36787944],\n",
       "       [0.36787944, 1.        , 0.04978707, 0.04978707]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5., -7.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2., -1.,  1.],\n",
       "       [ 1.,  2., -1., -1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [1]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-25fc0247abd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "predictions[target_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.],\n",
       "       [ 2.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.take(predictions, target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [1]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "return arrays must be of ArrayType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-0e86a5be3d8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: return arrays must be of ArrayType"
     ]
    }
   ],
   "source": [
    "np.add(predictions, target_index, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-afcdd4fab5ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "np.add.at(predictions, target_index, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2., -1.,  1.],\n",
       "       [ 1.,  2., -1., -1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [1]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.put_along_axis(predictions, target_index, 10, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2., 10.,  1.],\n",
       "       [ 1., 10., -1., -1.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs: [[0.20603191 0.56005279 0.02788339 0.20603191]]\n",
      "loss: 3.5797242232074917\n",
      "probs: [[0.20603027 0.56005395 0.02788344 0.20603233]]\n",
      "loss: 3.5797221628965787\n",
      "probs: [[0.20603355 0.56005164 0.02788333 0.20603148]]\n",
      "loss: 3.5797262835347627\n",
      "probs: [[0.20603306 0.56005033 0.02788354 0.20603306]]\n",
      "loss: 3.579718622691863\n",
      "probs: [[0.20603076 0.56005526 0.02788323 0.20603076]]\n",
      "loss: 3.5797298237477597\n",
      "probs: [[0.20603197 0.56005295 0.02788312 0.20603197]]\n",
      "loss: 3.579733944374979\n",
      "probs: [[0.20603185 0.56005264 0.02788366 0.20603185]]\n",
      "loss: 3.5797145020427146\n",
      "probs: [[0.20603233 0.56005395 0.02788344 0.20603027]]\n",
      "loss: 3.5797221628965787\n",
      "probs: [[0.20603148 0.56005164 0.02788333 0.20603355]]\n",
      "loss: 3.5797262835347627\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014661918673797\n",
      "probs: [[0.29692065 0.29692362 0.1092321  0.29692362]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014658949456809\n",
      "probs: [[0.29692483 0.29692186 0.10923145 0.29692186]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014664887911659\n",
      "probs: [[0.29692362 0.29692065 0.1092321  0.29692362]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014658949456809\n",
      "probs: [[0.29692186 0.29692483 0.10923145 0.29692186]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014664887911659\n",
      "probs: [[0.29692307 0.29692307 0.1092308  0.29692307]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014660826360935\n",
      "probs: [[0.29692242 0.29692242 0.10923275 0.29692242]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014663010996387\n",
      "probs: [[0.29692362 0.29692362 0.1092321  0.29692065]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.0146689494568095\n",
      "probs: [[0.29692186 0.29692186 0.10923145 0.29692483]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014654887911659\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216173 0.41362261 0.02059306 0.41362261]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014660397050031\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216431 0.41362135 0.02059299 0.41362135]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014663440310462\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216365 0.41361955 0.02059311 0.41362369]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014667782466159\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216239 0.4136244  0.02059294 0.41362027]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.0146560549056876\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216305 0.41362206 0.02059282 0.41362206]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014661712744548\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216299 0.41362189 0.02059323 0.41362189]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014662124605061\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216365 0.41362369 0.02059311 0.41361955]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014657782466159\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216239 0.41362027 0.02059294 0.4136244 ]\n",
      " [0.3994863  0.3994863  0.1469628  0.05406459]]\n",
      "loss: 5.014666054905687\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.39948391 0.3994879  0.14696339 0.05406481]]\n",
      "loss: 5.014657923822744\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.3994887  0.39948471 0.14696221 0.05406438]]\n",
      "loss: 5.014665913548837\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.3994879  0.39948391 0.14696339 0.05406481]]\n",
      "loss: 5.014657923822744\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.39948471 0.3994887  0.14696221 0.05406438]]\n",
      "loss: 5.014665913548837\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.39948689 0.39948689 0.14696154 0.05406467]]\n",
      "loss: 5.01466044905208\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.39948572 0.39948572 0.14696405 0.05406451]]\n",
      "loss: 5.014663388308049\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.39948652 0.39948652 0.14696288 0.05406408]]\n",
      "loss: 5.014671378030432\n",
      "probs: [[0.29692274 0.29692274 0.10923177 0.29692274]\n",
      " [0.15216302 0.41362198 0.02059303 0.41362198]\n",
      " [0.39948609 0.39948609 0.14696272 0.0540651 ]]\n",
      "loss: 5.014652459322276\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs: [[0.88079708 0.11920292]\n",
      " [0.04742587 0.95257413]]\n",
      "loss: 2.1755153626167147\n",
      "probs: [[0.88079708 0.11920292]\n",
      " [0.04742587 0.95257413]]\n",
      "loss: 2.1755153626167147\n",
      "probs: [[0.88079813 0.11920187]\n",
      " [0.04742587 0.95257413]]\n",
      "loss: 2.175524170592744\n",
      "probs: [[0.88079603 0.11920397]\n",
      " [0.04742587 0.95257413]]\n",
      "loss: 2.1755065546511845\n",
      "probs: [[0.88079603 0.11920397]\n",
      " [0.04742587 0.95257413]]\n",
      "loss: 2.1755065546511845\n",
      "probs: [[0.88079813 0.11920187]\n",
      " [0.04742587 0.95257413]]\n",
      "loss: 2.175524170592744\n",
      "probs: [[0.88079813 0.11920187]\n",
      " [0.04742542 0.95257458]]\n",
      "loss: 2.1755236963362714\n",
      "probs: [[0.88079603 0.11920397]\n",
      " [0.04742632 0.95257368]]\n",
      "loss: 2.175507028912175\n",
      "probs: [[0.88079603 0.11920397]\n",
      " [0.04742632 0.95257368]]\n",
      "loss: 2.175507028912175\n",
      "probs: [[0.88079813 0.11920187]\n",
      " [0.04742542 0.95257458]]\n",
      "loss: 2.1755236963362714\n",
      "probs: [[0.88079603 0.11920397]\n",
      " [0.04742542 0.95257458]]\n",
      "loss: 2.1755060803947117\n",
      "probs: [[0.88079813 0.11920187]\n",
      " [0.04742632 0.95257368]]\n",
      "loss: 2.1755246448537346\n",
      "probs: [[0.88079813 0.11920187]\n",
      " [0.04742632 0.95257368]]\n",
      "loss: 2.1755246448537346\n",
      "probs: [[0.88079603 0.11920397]\n",
      " [0.04742542 0.95257458]]\n",
      "loss: 2.1755060803947117\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 135.051445\n",
      "Epoch 1, loss: 244.647198\n",
      "Epoch 2, loss: 321.125156\n",
      "Epoch 3, loss: 353.861300\n",
      "Epoch 4, loss: 378.770278\n",
      "Epoch 5, loss: 414.726590\n",
      "Epoch 6, loss: 436.794349\n",
      "Epoch 7, loss: 445.463482\n",
      "Epoch 8, loss: 402.386398\n",
      "Epoch 9, loss: 405.338735\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11c874710>]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.162\n",
      "Epoch 0, loss: 383.833143\n",
      "Epoch 1, loss: 417.010872\n",
      "Epoch 2, loss: 413.329045\n",
      "Epoch 3, loss: 398.723558\n",
      "Epoch 4, loss: 412.163163\n",
      "Accuracy after training for 100 epochs:  0.181\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=5, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.005366\n",
      "Epoch 1, loss: 0.009406\n",
      "Epoch 2, loss: 0.013020\n",
      "Epoch 3, loss: 0.017112\n",
      "Epoch 4, loss: 0.020882\n",
      "Epoch 5, loss: 0.025208\n",
      "Epoch 6, loss: 0.029114\n",
      "Epoch 7, loss: 0.033027\n",
      "Epoch 8, loss: 0.036992\n",
      "Epoch 9, loss: 0.040525\n",
      "Epoch 10, loss: 0.044195\n",
      "Epoch 11, loss: 0.047355\n",
      "Epoch 12, loss: 0.050740\n",
      "Epoch 13, loss: 0.054748\n",
      "Epoch 14, loss: 0.058532\n",
      "Epoch 15, loss: 0.062225\n",
      "Epoch 16, loss: 0.066230\n",
      "Epoch 17, loss: 0.069458\n",
      "Epoch 18, loss: 0.073147\n",
      "Epoch 19, loss: 0.077460\n",
      "Epoch 0, loss: 0.008035\n",
      "Epoch 1, loss: 0.008397\n",
      "Epoch 2, loss: 0.008824\n",
      "Epoch 3, loss: 0.009187\n",
      "Epoch 4, loss: 0.009630\n",
      "Epoch 5, loss: 0.010048\n",
      "Epoch 6, loss: 0.010448\n",
      "Epoch 7, loss: 0.010840\n",
      "Epoch 8, loss: 0.011278\n",
      "Epoch 9, loss: 0.011738\n",
      "Epoch 10, loss: 0.012111\n",
      "Epoch 11, loss: 0.012530\n",
      "Epoch 12, loss: 0.012923\n",
      "Epoch 13, loss: 0.013314\n",
      "Epoch 14, loss: 0.013722\n",
      "Epoch 15, loss: 0.014203\n",
      "Epoch 16, loss: 0.014589\n",
      "Epoch 17, loss: 0.015075\n",
      "Epoch 18, loss: 0.015413\n",
      "Epoch 19, loss: 0.015782\n",
      "Epoch 0, loss: 0.001627\n",
      "Epoch 1, loss: 0.001673\n",
      "Epoch 2, loss: 0.001720\n",
      "Epoch 3, loss: 0.001762\n",
      "Epoch 4, loss: 0.001813\n",
      "Epoch 5, loss: 0.001853\n",
      "Epoch 6, loss: 0.001899\n",
      "Epoch 7, loss: 0.001946\n",
      "Epoch 8, loss: 0.001996\n",
      "Epoch 9, loss: 0.002039\n",
      "Epoch 10, loss: 0.002082\n",
      "Epoch 11, loss: 0.002127\n",
      "Epoch 12, loss: 0.002176\n",
      "Epoch 13, loss: 0.002227\n",
      "Epoch 14, loss: 0.002267\n",
      "Epoch 15, loss: 0.002310\n",
      "Epoch 16, loss: 0.002357\n",
      "Epoch 17, loss: 0.002409\n",
      "Epoch 18, loss: 0.002451\n",
      "Epoch 19, loss: 0.002491\n",
      "Epoch 0, loss: 0.250931\n",
      "Epoch 1, loss: 0.251067\n",
      "Epoch 2, loss: 0.251292\n",
      "Epoch 3, loss: 0.251545\n",
      "Epoch 4, loss: 0.251825\n",
      "Epoch 5, loss: 0.252069\n",
      "Epoch 6, loss: 0.252328\n",
      "Epoch 7, loss: 0.252586\n",
      "Epoch 8, loss: 0.252848\n",
      "Epoch 9, loss: 0.253135\n",
      "Epoch 10, loss: 0.253388\n",
      "Epoch 11, loss: 0.253645\n",
      "Epoch 12, loss: 0.253897\n",
      "Epoch 13, loss: 0.254175\n",
      "Epoch 14, loss: 0.254428\n",
      "Epoch 15, loss: 0.254677\n",
      "Epoch 16, loss: 0.254928\n",
      "Epoch 17, loss: 0.255196\n",
      "Epoch 18, loss: 0.255438\n",
      "Epoch 19, loss: 0.255681\n",
      "Epoch 0, loss: 0.025592\n",
      "Epoch 1, loss: 0.025616\n",
      "Epoch 2, loss: 0.025645\n",
      "Epoch 3, loss: 0.025671\n",
      "Epoch 4, loss: 0.025693\n",
      "Epoch 5, loss: 0.025720\n",
      "Epoch 6, loss: 0.025746\n",
      "Epoch 7, loss: 0.025772\n",
      "Epoch 8, loss: 0.025799\n",
      "Epoch 9, loss: 0.025823\n",
      "Epoch 10, loss: 0.025850\n",
      "Epoch 11, loss: 0.025877\n",
      "Epoch 12, loss: 0.025901\n",
      "Epoch 13, loss: 0.025928\n",
      "Epoch 14, loss: 0.025955\n",
      "Epoch 15, loss: 0.025980\n",
      "Epoch 16, loss: 0.026009\n",
      "Epoch 17, loss: 0.026035\n",
      "Epoch 18, loss: 0.026063\n",
      "Epoch 19, loss: 0.026089\n",
      "Epoch 0, loss: 0.002612\n",
      "Epoch 1, loss: 0.002615\n",
      "Epoch 2, loss: 0.002617\n",
      "Epoch 3, loss: 0.002620\n",
      "Epoch 4, loss: 0.002623\n",
      "Epoch 5, loss: 0.002626\n",
      "Epoch 6, loss: 0.002628\n",
      "Epoch 7, loss: 0.002631\n",
      "Epoch 8, loss: 0.002634\n",
      "Epoch 9, loss: 0.002637\n",
      "Epoch 10, loss: 0.002640\n",
      "Epoch 11, loss: 0.002643\n",
      "Epoch 12, loss: 0.002646\n",
      "Epoch 13, loss: 0.002649\n",
      "Epoch 14, loss: 0.002652\n",
      "Epoch 15, loss: 0.002655\n",
      "Epoch 16, loss: 0.002658\n",
      "Epoch 17, loss: 0.002661\n",
      "Epoch 18, loss: 0.002664\n",
      "Epoch 19, loss: 0.002667\n",
      "Epoch 0, loss: 0.266868\n",
      "Epoch 1, loss: 0.266897\n",
      "Epoch 2, loss: 0.266927\n",
      "Epoch 3, loss: 0.266957\n",
      "Epoch 4, loss: 0.266989\n",
      "Epoch 5, loss: 0.267017\n",
      "Epoch 6, loss: 0.267046\n",
      "Epoch 7, loss: 0.267078\n",
      "Epoch 8, loss: 0.267106\n",
      "Epoch 9, loss: 0.267139\n",
      "Epoch 10, loss: 0.267169\n",
      "Epoch 11, loss: 0.267198\n",
      "Epoch 12, loss: 0.267228\n",
      "Epoch 13, loss: 0.267260\n",
      "Epoch 14, loss: 0.267289\n",
      "Epoch 15, loss: 0.267321\n",
      "Epoch 16, loss: 0.267350\n",
      "Epoch 17, loss: 0.267382\n",
      "Epoch 18, loss: 0.267411\n",
      "Epoch 19, loss: 0.267442\n",
      "Epoch 0, loss: 0.026747\n",
      "Epoch 1, loss: 0.026750\n",
      "Epoch 2, loss: 0.026753\n",
      "Epoch 3, loss: 0.026756\n",
      "Epoch 4, loss: 0.026759\n",
      "Epoch 5, loss: 0.026762\n",
      "Epoch 6, loss: 0.026766\n",
      "Epoch 7, loss: 0.026769\n",
      "Epoch 8, loss: 0.026772\n",
      "Epoch 9, loss: 0.026775\n",
      "Epoch 10, loss: 0.026778\n",
      "Epoch 11, loss: 0.026781\n",
      "Epoch 12, loss: 0.026784\n",
      "Epoch 13, loss: 0.026787\n",
      "Epoch 14, loss: 0.026790\n",
      "Epoch 15, loss: 0.026793\n",
      "Epoch 16, loss: 0.026796\n",
      "Epoch 17, loss: 0.026799\n",
      "Epoch 18, loss: 0.026802\n",
      "Epoch 19, loss: 0.026805\n",
      "Epoch 0, loss: 0.002681\n",
      "Epoch 1, loss: 0.002681\n",
      "Epoch 2, loss: 0.002681\n",
      "Epoch 3, loss: 0.002682\n",
      "Epoch 4, loss: 0.002682\n",
      "Epoch 5, loss: 0.002682\n",
      "Epoch 6, loss: 0.002683\n",
      "Epoch 7, loss: 0.002683\n",
      "Epoch 8, loss: 0.002683\n",
      "Epoch 9, loss: 0.002684\n",
      "Epoch 10, loss: 0.002684\n",
      "Epoch 11, loss: 0.002684\n",
      "Epoch 12, loss: 0.002685\n",
      "Epoch 13, loss: 0.002685\n",
      "Epoch 14, loss: 0.002685\n",
      "Epoch 15, loss: 0.002685\n",
      "Epoch 16, loss: 0.002686\n",
      "Epoch 17, loss: 0.002686\n",
      "Epoch 18, loss: 0.002686\n",
      "Epoch 19, loss: 0.002687\n",
      "best validation accuracy achieved: 0.257000\n",
      "CPU times: user 1min 34s, sys: 7.85 s, total: 1min 42s\n",
      "Wall time: 58.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from itertools import product\n",
    "\n",
    "num_epochs = 20 #200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for learning_rate, reg_strength in product(learning_rates, reg_strengths):\n",
    "    classifier.fit(train_X, train_y, \n",
    "                   epochs=num_epochs, \n",
    "                   learning_rate=learning_rate, \n",
    "                   batch_size=batch_size, \n",
    "                   reg=reg_strength)\n",
    "    \n",
    "    pred = classifier.predict(val_X)\n",
    "    accuracy = multiclass_accuracy(pred, val_y)\n",
    "    \n",
    "    if accuracy > best_val_accuracy:\n",
    "        best_classifier = classifier\n",
    "        best_val_accuracy = accuracy\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.205000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
